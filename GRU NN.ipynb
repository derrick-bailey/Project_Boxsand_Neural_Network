{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHANGELOG  \n",
    "### Done:  \n",
    "Batch size 32 to 1  \n",
    "Made x- and y-axis same size  \n",
    "Student validation: catches if students amongst data are not identical  \n",
    "Data import rewrittten, now pulling all .csv files from a given folder and initializing dataframes  \n",
    "Initialized institutional data feature dataframe\n",
    "Initializing clickstream feature dataframe  \n",
    "Demographics  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Male / Female  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1st Gen / Non  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;White / Other  \n",
    "Experiment w/ neural net depth, activation functions, loss, optimizations  \n",
    "Neural net fitting \n",
    "### In Progress:  \n",
    "splitting for train/test w/ week 11 and week 3/4   \n",
    "Permutation Importance  \n",
    "~~Stochastic Hyperparameter Optimization~~  \n",
    "Feature Correlation\n",
    "\n",
    "### To Do:   \n",
    "n/a   \n",
    "#### Data:  \n",
    "Total Boxsand Clicks  \n",
    "Total Syllabus Clicks  \n",
    "Total Exam Access  \n",
    "Total Solutions Access  \n",
    "Total Practice Problems  \n",
    "Total Calendar Access  \n",
    "Total YouTube Access  \n",
    "Video Score Breakdown  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Quartile Division  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sum Quartile  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Avg var after sum  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Quartile Weight?  \n",
    "Prelab + Lab?  \n",
    "Total Openstax Access  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "from tensorflow.keras.layers import Dropout\n",
    "import tensorboard\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "#Clear session and set random seeds to static\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def import_data(): \n",
    "    path = r'C:\\Users\\OSU_bailderr\\Box\\Anon Data\\Undergrad Students\\Derrick Bailey\\NN Data\\week11'\n",
    "    file = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "    onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "    dfs = []\n",
    "    for f in file:\n",
    "        df = pd.read_csv(f)\n",
    "        dfs.append(df)\n",
    "    dfs = np.array(dfs, dtype=object)\n",
    "\n",
    "    institutional_Data = dfs[0]\n",
    "\n",
    "    gradebook = dfs[1]\n",
    "#     print(gradebook['M2%'])\n",
    "    #drop students who do have missing exam scores\n",
    "    gradebook['M1%'].replace('', 0, inplace=True)\n",
    "    gradebook['M2%'].replace('', 0, inplace=True)\n",
    "    gradebook['Final%'].replace('', 0, inplace=True)\n",
    "    \n",
    "    gradebook = gradebook.dropna(axis=0, how='all')\n",
    "    \n",
    "    gradebook = gradebook[gradebook['M1%'] != 0]\n",
    "    gradebook = gradebook[gradebook['M2%'] != 0]\n",
    "    gradebook = gradebook[gradebook['Final%'] != 0]\n",
    "#     display(gradebook)\n",
    "\n",
    "    clickstream1 = dfs[2] \n",
    "    clickstream2 = dfs[3]\n",
    "\n",
    "    headers = list(clickstream1)\n",
    "    clickstream2.columns = headers\n",
    "    clickstream = pd.concat([clickstream1, clickstream2], ignore_index=False)\n",
    "\n",
    "    institutional_Data_Students = institutional_Data['ID'].unique()\n",
    "    institutional_Data_Students = pd.DataFrame(institutional_Data_Students)\n",
    "    institutional_Data_Students = institutional_Data_Students.dropna(axis=0, how='all')\n",
    "    institutional_Data_Students.columns = ['ID']\n",
    "\n",
    "    gradebook_Students = gradebook['ID'].unique()\n",
    "    gradebook_Students = pd.DataFrame(gradebook_Students)\n",
    "    gradebook_Students = gradebook_Students.dropna(axis=0, how='all')\n",
    "    gradebook_Students.columns = ['ID']\n",
    "\n",
    "    clickstream_Students = clickstream['ID'].unique()\n",
    "    clickstream_Students = pd.DataFrame(clickstream_Students)\n",
    "    clickstream_Students = clickstream_Students.dropna(axis=0, how='all')\n",
    "    clickstream_Students.columns = ['ID']\n",
    "\n",
    "    #delete students in clickstream and institutional if not present in gradebook\n",
    "    institutional_Data_Students = institutional_Data_Students[institutional_Data_Students.ID.isin(gradebook_Students.ID)].dropna()\n",
    "    clickstream_Students = clickstream_Students[clickstream_Students.ID.isin(gradebook_Students.ID)].dropna()\n",
    "\n",
    "    #check that students in institutional_Data match students in clickstream\n",
    "    class StudentError(Exception):\n",
    "        pass\n",
    "    try:\n",
    "        student_Check = clickstream_Students.iloc[:,0].str.strip().str.lower().unique().all() == institutional_Data_Students.iloc[:,0].str.strip().str.lower().unique().all() == gradebook_Students.iloc[:,0].str.strip().str.lower().unique().all()\n",
    "        if student_Check != True:\n",
    "            raise StudentError\n",
    "    except StudentError:\n",
    "        print(\"Students in Dataframes do not match!\")\n",
    "        assert False\n",
    "    except(SyntaxError):\n",
    "        print(\"Syntax Issue!\")\n",
    "        assert False\n",
    "\n",
    "    #Collect demographics\n",
    "    #assert demographic\n",
    "#     demo = \"Y\"\n",
    "#     institutional_Data = institutional_Data[institutional_Data[\"FIRST GEN IND\"].str.contains(demo)==False]\n",
    "    \n",
    "#     display(institutional_Data)\n",
    "    \n",
    "    #Parse through Institutional Data to generate featureset\n",
    "    institutional_Data_cols = list(institutional_Data.columns)\n",
    "    # drop any and all columns that are all ZERO (necessary: columns of zeroes will influence weighting and prediction)\n",
    "    institutional_Data = institutional_Data.loc[:, (institutional_Data !=0).any(axis=0)]\n",
    "    # drop any and all columns that are all NaN (necessary step - matrix operation on NaN will result in all operations NaN)\n",
    "    institutional_Data = institutional_Data.dropna(axis=1, how='all')\n",
    "    # fill NaN with zeroes\n",
    "    institutional_Data = institutional_Data.fillna(0)\n",
    "    institutional_Data = institutional_Data[institutional_Data.ID.isin(institutional_Data_Students.ID)]\n",
    "    institutional_Data_Features = [\"ID\",\"OVERALL OSU GPA\",\"OSU GPA\",\"OSU CREDITS ATTEMPT\",\"OSU CREDITS EARNED\"]\n",
    "    institutional_Data = institutional_Data.loc[:, institutional_Data_Features].copy()\n",
    "    \n",
    "    #delete students in clickstream and gradebook if not present in institutional_data (demographic filter)\n",
    "    institutional_Data_Students = institutional_Data_Students[institutional_Data_Students.ID.isin(institutional_Data.ID)].dropna()\n",
    "    clickstream_Students = clickstream_Students[clickstream_Students.ID.isin(institutional_Data.ID)].dropna()\n",
    "    \n",
    "    institutional_Data = institutional_Data[institutional_Data.ID.isin(institutional_Data_Students.ID)].dropna()\n",
    "    clickstream = clickstream[clickstream.ID.isin(clickstream_Students.ID)].dropna()\n",
    "    \n",
    "    #recatch students\n",
    "    institutional_Data_Students = institutional_Data['ID'].unique()\n",
    "    institutional_Data_Students = pd.DataFrame(institutional_Data_Students)\n",
    "    institutional_Data_Students = institutional_Data_Students.dropna(axis=0, how='all')\n",
    "    institutional_Data_Students.columns = ['ID']\n",
    "\n",
    "    gradebook_Students = gradebook['ID'].unique()\n",
    "    gradebook_Students = pd.DataFrame(gradebook_Students)\n",
    "    gradebook_Students = gradebook_Students.dropna(axis=0, how='all')\n",
    "    gradebook_Students.columns = ['ID']\n",
    "\n",
    "    clickstream_Students = clickstream['ID'].unique()\n",
    "    clickstream_Students = pd.DataFrame(clickstream_Students)\n",
    "    clickstream_Students = clickstream_Students.dropna(axis=0, how='all')\n",
    "    clickstream_Students.columns = ['ID']\n",
    "    \n",
    "    #Parse through Gradebook Data to generate featureset\n",
    "    gradebook_cols = list(gradebook.columns)\n",
    "    #feature list for gradebook\n",
    "    gradebook_features = ['ID', 'raw','online','HW%','RW%','Rec %','Lab %','M1%','M2%','Final%','Overall Grade %']\n",
    "    gradebook = gradebook[gradebook_features]\n",
    "    del gradebook['M1%']\n",
    "    del gradebook['M2%']\n",
    "    del gradebook['Final%']\n",
    "    gradebook = gradebook[gradebook.ID.isin(institutional_Data.ID)].dropna()\n",
    "    final_Grade = gradebook['Overall Grade %']\n",
    "    final_Grade = pd.DataFrame([final_Grade]).T\n",
    "    final_Grade = final_Grade.values\n",
    "    final_Grade = pd.DataFrame(final_Grade)\n",
    "    # remove final grade from gradebook\n",
    "    del gradebook['Overall Grade %']\n",
    "    # drop any and all columns that are all ZERO (necessary: columns of zeroes will influence weighting and prediction)\n",
    "    gradebook = gradebook.loc[:, (gradebook !=0).any(axis=0)]\n",
    "    # drop any and all columns that are all NaN (necessary step - matrix operation on NaN will result in all operations NaN)\n",
    "    gradebook = gradebook.dropna(axis=1, how='all')\n",
    "    # fill NaN with zeroes\n",
    "    gradebook = gradebook.fillna(0)\n",
    "    # recatch cols var\n",
    "    cols = list(gradebook.columns)\n",
    "    gradebook = gradebook[gradebook.ID.isin(gradebook_Students.ID)]\n",
    "\n",
    "    #Parse through Clickstream Data to generate featureset\n",
    "    clickstream_cols = list(clickstream.columns)\n",
    "    clickstream.set_index=clickstream['ID']\n",
    "    # drop any and all columns that are all ZERO (necessary: columns of zeroes will influence weighting and prediction)\n",
    "    clickstream = clickstream.loc[:, (clickstream !=0).any(axis=0)]\n",
    "    # drop any and all columns that are all NaN (necessary step - matrix operation on NaN will result in all operations NaN)\n",
    "    clickstream = clickstream.dropna(axis=1, how='all')\n",
    "    # fill NaN with zeroes\n",
    "    clickstream = clickstream.fillna(0)\n",
    "    clickstream = clickstream[clickstream.ID.isin(clickstream_Students.ID)]\n",
    "\n",
    "    # now to parse through for specific features\n",
    "    # create list out of student column\n",
    "    student_list = clickstream[['ID']]\n",
    "    student_list_unique = pd.DataFrame(student_list['ID'].unique())\n",
    "    student_list_unique.columns = ['ID']\n",
    "    clickstream_features = ['m1_sols', 'm2_sols','f_sols', '#Practice', '#Fundamental', 'Calendar', 'KALTURA', 'youtube']\n",
    "    clickstream_cut = clickstream[clickstream['Page title'].str.contains(\"|\".join(clickstream_features))]\n",
    "    \n",
    "    studentdf = []\n",
    "    studentdf = [x for _, x in clickstream_cut.groupby(by='ID')]\n",
    "    \n",
    "    clickstream_Data = []\n",
    "    clickstream_Data = pd.DataFrame(columns = clickstream_features)\n",
    "    clickstream_Data = student_list_unique.append(clickstream_Data)\n",
    "\n",
    "    count = []\n",
    "\n",
    "    #automated feature-by-feature approach\n",
    "    for feature in clickstream_features:\n",
    "        for pos in studentdf:\n",
    "            for ID in pos['ID'].unique():\n",
    "                ID = pos['ID'].unique()\n",
    "                count.append(pos['Page title'].str.contains(feature).sum())\n",
    "        clickstream_Data[feature] = count\n",
    "        count.clear()\n",
    "\n",
    "    frames = pd.merge(institutional_Data, gradebook, on='ID')\n",
    "    frames = pd.merge(frames, clickstream_Data, on='ID')\n",
    "\n",
    "#     final_Grade = final_Grade[final_Grade.ID.isin(institutional_Data_Students.ID)].dropna()\n",
    "    \n",
    "    input_Features = frames\n",
    "    feature_num = len(input_Features.columns)\n",
    "    # display(input_Features)\n",
    "    return input_Features, final_Grade, studentdf, student_list_unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def debug(debug=False):\n",
    "    while debug == True:\n",
    "        debug_input = int(input(\"Invoke the rite of passage! \\n \\\n",
    "        1: Final Grades Dataframe \\n \\\n",
    "        2: Student Dataframe \\n \\\n",
    "        3: Input Features Dataframe \\n \\\n",
    "        4: Exit \\n\"))\n",
    "        if debug_input == 1:\n",
    "            print(\"Final Grades Dataframe\")\n",
    "            display(final_Grade)\n",
    "            break\n",
    "        elif debug_input == 2:\n",
    "            print(\"Student Dataframe\")\n",
    "            display(studentdf)\n",
    "            break\n",
    "        elif debug_input == 3:\n",
    "            print(\"Input Features Dataframe\")\n",
    "            display(input_Features)\n",
    "            break\n",
    "        elif debug_input == 4:\n",
    "            debug_input = False\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def process_data(input_Features, final_Grade):\n",
    "    # Split data for train/test\n",
    "    x_train, x_test, y_train, y_test = train_test_split(input_Features, final_Grade, \n",
    "                                                        test_size=0.33, random_state=0)\n",
    "    # test\n",
    "    x_train = np.array(x_train)\n",
    "    x_test = np.array(x_test)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    # delete student column before normalization but saving student column\n",
    "    x_train_student = x_train[:,0]\n",
    "    x_test_student = x_test[:,0]\n",
    "    x_train = np.delete(x_train, 0, 1)\n",
    "    x_test = np.delete(x_test, 0, 1)\n",
    "    \n",
    "    # Normalization\n",
    "    sc = MinMaxScaler(feature_range = (0,1))\n",
    "    x_train_norm = sc.fit_transform(x_train)\n",
    "    x_test_norm = sc.fit_transform(x_test)\n",
    "    y_train_norm = sc.fit_transform(y_train)\n",
    "    y_test_norm = sc.fit_transform(y_test)\n",
    "\n",
    "    x_train_norm = np.reshape(x_train_norm, (x_train_norm.shape[0], x_train_norm.shape[1], 1))\n",
    "    x_test_norm = np.reshape(x_test_norm, (x_test_norm.shape[0], x_test_norm.shape[1], 1))\n",
    "    \n",
    "    return x_train_norm, x_test_norm, y_train_norm, y_test_norm, x_train_student, x_test_student, sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def student_cross_ref(student_list_unique, final_Grade, predicted_Grade):\n",
    "    # We must cross-reference the split data with the final grade master list\n",
    "    # and pull only the matching student numbers from the final grade master list.\n",
    "\n",
    "    # Start by zipping together student dataframe and final grade dataframe.\n",
    "    final_Grade = np.hstack((student_list_unique, final_Grade))\n",
    "    final_Grade = pd.DataFrame(final_Grade)\n",
    "    # To make our lives easy, let's name the columns for our predicted and final grade dataframes.\n",
    "    final_Grade.columns = ['FStudent','FGrade']\n",
    "    predicted_Grade.columns = ['PStudent','PGrade']\n",
    "    # Now, we compare the student columns for these dataframes.\n",
    "    merged_Grades = final_Grade.merge(predicted_Grade, left_on = 'FStudent', right_on = 'PStudent', how = 'left')\n",
    "    # with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    #     display(merged_Grades)\n",
    "    merged_Grades = merged_Grades[merged_Grades['PStudent'].notna()]\n",
    "    return merged_Grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_optimization(hyp_opt):   \n",
    "    if hyp_opt == \"Y\":\n",
    "        param_list = [[*range(100,1100,100)], \n",
    "                      [*map(lambda x: 2 ** x, range(8,-1,-1))], \n",
    "                      [*map(lambda x: 1*10**(-x), range(5))], \n",
    "#                       [*map(lambda x: 1*10**(-1-x), range(5))], \n",
    "                      list(np.linspace(0.05,0.3,6).round(decimals=2)),\n",
    "                      list(np.linspace(0.05,0.3,6).round(decimals=2)), \n",
    "                      list(np.linspace(0.05,0.3,6).round(decimals=2)), \n",
    "                      list(np.linspace(0.05,0.3,6).round(decimals=2)), \n",
    "                      list(np.linspace(0.05,0.3,6).round(decimals=2)), \n",
    "                      list(np.linspace(0.05,0.3,6).round(decimals=2)),\n",
    "                      [*map(lambda x: 2 ** x, range(10))],\n",
    "                      [*map(lambda x: 2 ** x, range(10))], \n",
    "                      [*map(lambda x: 2 ** x, range(10))]]\n",
    "        hype_var_list_index = ['num_epochs', 'batch_size', 'lr', 'recurrent_dropout1', 'recurrent_dropout2', 'recurrent_dropout3', 'dropout1', 'dropout2', 'dropout3', 'gru_units1', 'gru_units2', 'gru_units3']\n",
    "\n",
    "        num_epochs, batch_size, lr, recurrent_dropout1, recurrent_dropout2, recurrent_dropout3, dropout1, dropout2, dropout3, gru_units1, gru_units2, gru_units3 = [param_list[i] for i in range(len(param_list))]\n",
    "        hype_var_list = pd.DataFrame(index=hype_var_list_index,\n",
    "                                      data=([num_epochs], \n",
    "                                      [batch_size], \n",
    "                                      [lr], \n",
    "                                      [recurrent_dropout1], \n",
    "                                      [recurrent_dropout2], \n",
    "                                      [recurrent_dropout3], \n",
    "                                      [dropout1], \n",
    "                                      [dropout2], \n",
    "                                      [dropout3], \n",
    "                                      [gru_units1], \n",
    "                                      [gru_units2], \n",
    "                                      [gru_units3]))\n",
    "        optimize_gate = True\n",
    "#         hype_var_list.DataFrame()\n",
    "#         print(hype_var_list)\n",
    "#         print(hype_var_list)\n",
    "        return hype_var_list, optimize_gate\n",
    "    if hyp_opt == \"N\":\n",
    "        param_list = [500, 1, 1e-4, 0.15, 0.15, 0.15, 0.2, 0.2, 0.2, 3125, 625, 125]\n",
    "        num_epochs, batch_size, lr, recurrent_dropout1, recurrent_dropout2, recurrent_dropout3, dropout1, dropout2, dropout3, gru_units1, gru_units2, gru_units3 = [param_list[i] for i in range(len(param_list))]\n",
    "        hype_var_list = [num_epochs, \n",
    "                         batch_size, \n",
    "                         lr, \n",
    "                         recurrent_dropout1, \n",
    "                         recurrent_dropout2, \n",
    "                         recurrent_dropout3, \n",
    "                         dropout1, \n",
    "                         dropout2, \n",
    "                         dropout3, \n",
    "                         gru_units1, \n",
    "                         gru_units2, \n",
    "                         gru_units3]\n",
    "        optimize_gate = False\n",
    "        return hype_var_list, optimize_gate\n",
    "    if hyp_opt != \"Y\" and hyp_opt != \"N\":\n",
    "        print(\"Try Again!\")\n",
    "        hyp_opt = hyperparameter_optimization(input(\"Optimize? \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateModel(hype_var_list, optimize_gate, x_train_norm, y_train_norm, x_test_norm, x_test_student, sc, student_list_unique, final_Grade):\n",
    "    if optimize_gate == True:\n",
    "        index = hype_var_list.index\n",
    "        for i in hype_var_list:\n",
    "            num_epochs, batch_size, lr, recurrent_dropout1, recurrent_dropout2, recurrent_dropout3, dropout1, dropout2, dropout3, gru_units1, gru_units2, gru_units3 = hype_var_list[i]\n",
    "        for a in num_epochs:\n",
    "#             print(\"num_epochs: \", a, \"\\n\")\n",
    "            for b in batch_size:\n",
    "#                 print(\"batch_size: \", b, \"\\n\")\n",
    "                for c in lr:\n",
    "#                     print(\"lr: \", c, \"\\n\")\n",
    "#                         print(\"decay: \", d, \"\\n\")\n",
    "                        for d in recurrent_dropout1:\n",
    "#                             print(\"recurrent_dropout1: \", e, \"\\n\")\n",
    "                            for e in recurrent_dropout2:\n",
    "#                                 print(\"recurrent_dropout2: \", f, \"\\n\")\n",
    "                                for f in recurrent_dropout3:\n",
    "#                                     print(\"recurrent_dropout3: \", g, \"\\n\")\n",
    "                                    for g in dropout1:\n",
    "#                                         print(\"dropout1: \", h, \"\\n\")\n",
    "                                        for h in dropout2:\n",
    "#                                             print(\"dropout2: \", i, \"\\n\")\n",
    "                                            for i in dropout3:\n",
    "#                                                 print(\"dropout3: \", j, \"\\n\")\n",
    "                                                for j in gru_units1:\n",
    "#                                                     print(\"gru_units1: \", k, \"\\n\")\n",
    "                                                    for k in gru_units2:\n",
    "#                                                         print(\"gru_units2: \", l, \"\\n\")\n",
    "                                                        for l in gru_units3:\n",
    "#                                                             print(\"gru_units3: \", m, \"\\n\")\n",
    "                                                            hype_list = [a,b,c,d,e,f,g,h,i,j,k,l]\n",
    "                                                            BuildModel(x_train_norm, y_train_norm, x_test_norm, x_test_student, sc, hype_list, student_list_unique, final_Grade)\n",
    "        #BuildModel()\n",
    "#         pass\n",
    "    if optimize_gate == False:\n",
    "        #train on defaults\n",
    "#         print(hype_var_list)\n",
    "#         for i in hype_var_list:\n",
    "        num_epochs, batch_size, lr, recurrent_dropout1, recurrent_dropout2, recurrent_dropout3, dropout1, dropout2, dropout3, gru_units1, gru_units2, gru_units3 = hype_var_list\n",
    "#         print(num_epochs, batch_size, lr, recurrent_dropout1, recurrent_dropout2, recurrent_dropout3, dropout1, dropout2, dropout3, gru_units1, gru_units2, gru_units3)\n",
    "        hype_list = [num_epochs, batch_size, lr, recurrent_dropout1, recurrent_dropout2, recurrent_dropout3, dropout1, dropout2, dropout3, gru_units1, gru_units2, gru_units3]\n",
    "        BuildModel(x_train_norm, y_train_norm, x_test_norm, x_test_student, sc, hype_list, student_list_unique, final_Grade)\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_chaos(merged_Grades, loss_df, mae, mse, fit):\n",
    "    # Visualize the chaos\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2)\n",
    "    ax1.scatter(merged_Grades.iloc[:,3],merged_Grades.iloc[:,1], c=merged_Grades.iloc[:,3], cmap = 'jet_r', edgecolors='black')\n",
    "    ax1.set_xlabel('Predicted Grade')\n",
    "    ax1.set_ylabel('Actual Grade')\n",
    "    min_predicted = merged_Grades['PGrade'].min()\n",
    "    ax1.set_xlim(0,100)\n",
    "    ax1.set_ylim(0,100)\n",
    "\n",
    "    ax2.plot(loss_df[['loss', 'val_loss']])\n",
    "#     print(loss_df)\n",
    "    ax2.set_xlabel(\"Number of Epochs\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "    ax2.plot(fit.history['loss'], label = 'train')\n",
    "    ax2.plot(fit.history['val_loss'], label = 'test')\n",
    "    ax2.legend()\n",
    "    textstr = '\\n'.join((\n",
    "        r'$MAE = %1.3f$' %mae,\n",
    "        r'$MSE = %1.3f$' %mse))\n",
    "\n",
    "    ax1.text(0.05, 0.95, textstr, transform = ax1.transAxes, fontsize = 14, va = 'top')\n",
    "\n",
    "    lims1 = [\n",
    "        np.min([ax1.get_xlim(), ax1.get_ylim()]),  # min of both axes\n",
    "        np.max([ax1.get_xlim(), ax1.get_ylim()]),  # max of both axes\n",
    "    ]\n",
    "\n",
    "    lims2 = [\n",
    "         np.min([ax2.get_xlim(), ax2.get_ylim()]),  # min of both axes\n",
    "         np.max([ax2.get_xlim(), ax2.get_ylim()]),  # max of both axes\n",
    "    ]\n",
    "\n",
    "    # now plot both limits against eachother\n",
    "    ax1.plot(lims1, lims1, 'k-', alpha=0.5, zorder=0)\n",
    "\n",
    "    ax1.grid()\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(right=2, wspace=0.2)\n",
    "    \n",
    "#     datetime = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#     fig.savefig(\"Figure \"+ datetime.str() + \".png\")\n",
    "    \n",
    "    plt.show()\n",
    "    figname = input('Graph name: ')\n",
    "    fig.savefig(\"Graphs/{}.svg\".format(figname), transparent=False, facecolor = 'w', bbox_inches='tight')\n",
    "    fig.savefig(\"Graphs/{}.png\".format(figname), transparent=False, facecolor = 'w', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_heatmap(train):\n",
    "    train = pd.DataFrame(train, columns=cols)\n",
    "    correlations = train.corr()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    sns.heatmap(correlations, vmax=1.0, vmin=-1.0, center=0, fmt='.2f',\n",
    "                square=True, linewidths=1, annot=True, cmap = 'vlag', cbar_kws={\"shrink\": .70})\n",
    "    savegraph = input('Do you want to save this graph? Y/N: ')\n",
    "    if savegraph == \"Y\":\n",
    "        figname = input('Graph name: ')\n",
    "        fig.savefig(\"Graphs/{}.svg\".format(figname), transparent=False, facecolor = 'w', dpi=150)\n",
    "        fig.savefig(\"Graphs/{}.png\".format(figname), transparent=False, facecolor = 'w', dpi=150)\n",
    "    plt.show();\n",
    "# input_Features, final_Grade, studentdf, student_list_unique = import_data()\n",
    "# cols = list(input_Features.keys())\n",
    "# cols.pop(0)\n",
    "# x_train_norm, x_test_norm, y_train_norm, y_test_norm, x_train_student, x_test_student, sc = process_data(input_Features, final_Grade)\n",
    "# x_train_norm = pd.DataFrame(x_train_norm, columns=cols)\n",
    "# y_train_norm = pd.DataFrame(y_train_norm, columns=['final_Grade'])\n",
    "# combined = pd.concat([x_train_norm, y_train_norm], axis=1)\n",
    "# cols.append('final_Grade')\n",
    "# correlation_heatmap(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Permution Importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savegraph = input('Do you want to save this graph? Y/N: ')\n",
    "# if savegraph == \"Y\":\n",
    "#     figname = input('Graph name: ')\n",
    "#     fig.savefig(\"Graphs/{}.svg\".format(figname), transparent=False, facecolor = 'w', dpi=150)\n",
    "#     fig.savefig(\"Graphs/{}.png\".format(figname), transparent=False, facecolor = 'w', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_Students = input(\"Display students? Y/N: \")\n",
    "# display_Students.upper()\n",
    "# if display_Students == \"Y\":\n",
    "#     with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#         display(merged_Grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def BuildModel(x_train_norm, y_train_norm, x_test_norm, x_test_student, sc, hype_list, student_list_unique, final_Grade):\n",
    "    num_epochs, batch_size, lr, recurrent_dropout1, recurrent_dropout2, recurrent_dropout3, dropout1, dropout2, dropout3, gru_units1, gru_units2, gru_units3 = hype_list\n",
    "#     print(num_epochs, batch_size, lr, recurrent_dropout1, recurrent_dropout2, recurrent_dropout3, dropout1, dropout2, dropout3, gru_units1, gru_units2, gru_units3)\n",
    "    print(x_train_norm.shape)\n",
    "    #GRU model\n",
    "#     type(x_train_norm)\n",
    "    tf.keras.backend.set_floatx('float64')\n",
    "    with tf.device('/cpu:0'):\n",
    "        # NN Parameters\n",
    "        # Initialization\n",
    "        RNN = Sequential()\n",
    "        start = time.perf_counter()\n",
    "        # 1st GRU layer & Dropout regularisation\n",
    "        RNN.add(GRU(units=gru_units1, return_sequences=True, recurrent_dropout=recurrent_dropout1, input_shape=(x_train_norm.shape[1], 1)))\n",
    "        RNN.add(Dropout(dropout1))\n",
    "#         RNN.add(BatchNormalization())\n",
    "        # 2nd GRU Layer & Dropout regularisation\n",
    "        RNN.add(GRU(units=gru_units2, return_sequences=True, recurrent_dropout=recurrent_dropout2))\n",
    "        RNN.add(Dropout(dropout2))\n",
    "#         RNN.add(BatchNormalization())\n",
    "        # 3rd GRU Layer & Dropout regularisation\n",
    "        RNN.add(GRU(units=gru_units3, recurrent_dropout=recurrent_dropout3))\n",
    "        RNN.add(Dropout(dropout3))\n",
    "#         RNN.add(BatchNormalization())\n",
    "        # 4th GRU Layer\n",
    "#         RNN.add(GRU(units=50, recurrent_dropout=0.1))\n",
    "    #   RNN.add(BatchNormalization())\n",
    "        # Output Layer\n",
    "        RNN.add(Dense(units = 1))\n",
    "        # Compiling RNN\n",
    "        loss = tf.keras.losses.MeanSquaredError()\n",
    "        decayed_lr = tf.keras.optimizers.schedules.ExponentialDecay(lr, 1000, 0.96, staircase=False)\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=decayed_lr)\n",
    "        RNN.compile(optimizer=opt, loss=loss)\n",
    "\n",
    "        #Early Stop Callback\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "        min_delta = 5e-6, \n",
    "        patience=30, \n",
    "        restore_best_weights = True)\n",
    "        #Checkpoint Callback\n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=r'C:\\Users\\OSU_bailderr\\Box\\Anon Data\\Undergrad Students\\Derrick Bailey\\NN Repository\\Checkpoints',\n",
    "        save_weights_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True)\n",
    "        #Tensorboard Callback\n",
    "        logdir='logs\\\\fit\\\\' + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard = keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)\n",
    "\n",
    "        # Fitting RNN to training set\n",
    "        fit = RNN.fit(x_train_norm, y_train_norm, epochs = num_epochs, batch_size = batch_size, validation_split=0.15, validation_data=None, callbacks=[early_stop, tensorboard], shuffle=True)\n",
    "        loss_df = pd.DataFrame(fit.history)\n",
    "        print(RNN.summary())\n",
    "    elapsed = time.perf_counter() - start\n",
    "    print('Elapsed %.3f seconds.' % elapsed)\n",
    "    # Evaluate mode\n",
    "    # history = RNN.evaluate(x_test_norm, y_test_norm, batch_size = 1)\n",
    "\n",
    "    # Predicted Grade\n",
    "    predicted_Grade = RNN.predict(x_test_norm)\n",
    "    # predicted_Grade_Dataset = np.zeros(shape=(len(predicted_Grade), 10))\n",
    "    # predicted_Grade_Dataset[:,0] = predicted_Grade[:,0]\n",
    "    predicted_Grade = sc.inverse_transform(predicted_Grade)[:,0]\n",
    "    predicted_Grade = np.vstack((x_test_student, predicted_Grade))\n",
    "    predicted_Grade = predicted_Grade.T\n",
    "    predicted_Grade = pd.DataFrame(predicted_Grade)\n",
    "    \n",
    "    \n",
    "#     return predicted_Grade, loss_df\n",
    "\n",
    "    merged_Grades = student_cross_ref(student_list_unique, final_Grade, predicted_Grade)\n",
    "    #compute MAE & MSE\n",
    "    mae = tf.keras.losses.MeanAbsoluteError()\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    #write MAE from scratch, compare to tf???\n",
    "    mae = mae(merged_Grades.iloc[:,3],merged_Grades.iloc[:,1]).numpy()\n",
    "    mse = mse(merged_Grades.iloc[:,3],merged_Grades.iloc[:,1]).numpy()\n",
    "    visualize_chaos(merged_Grades, loss_df, mae, mse, fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\OSU_bailderr\\\\Box\\\\Anon Data\\\\Undergrad Students\\\\Derrick Bailey\\\\NN Data\\\\week11'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-728c4b2d94e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#     predicted_Grade, loss_df = NeuralNetwork(x_train_norm, y_train_norm, x_test_norm, x_test_student, sc)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-728c4b2d94e7>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0minput_Features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_Grade\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudentdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudent_list_unique\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mx_train_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train_student\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test_student\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_Features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_Grade\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mhype_var_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimize_gate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhyperparameter_optimization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhyp_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Optimize Hyperparameters? Y/N: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mEvaluateModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhype_var_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimize_gate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test_student\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudent_list_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_Grade\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-c032869da00a>\u001b[0m in \u001b[0;36mimport_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'C:\\Users\\OSU_bailderr\\Box\\Anon Data\\Undergrad Students\\Derrick Bailey\\NN Data\\week11'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"*.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0monlyfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\OSU_bailderr\\\\Box\\\\Anon Data\\\\Undergrad Students\\\\Derrick Bailey\\\\NN Data\\\\week11'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    input_Features, final_Grade, studentdf, student_list_unique = import_data()\n",
    "    x_train_norm, x_test_norm, y_train_norm, y_test_norm, x_train_student, x_test_student, sc = process_data(input_Features, final_Grade)\n",
    "    hype_var_list, optimize_gate = hyperparameter_optimization(hyp_opt = input(\"Optimize Hyperparameters? Y/N: \"))\n",
    "    EvaluateModel(hype_var_list, optimize_gate, x_train_norm, y_train_norm, x_test_norm, x_test_student, sc, student_list_unique, final_Grade)\n",
    "#     predicted_Grade, loss_df = NeuralNetwork(x_train_norm, y_train_norm, x_test_norm, x_test_student, sc)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
