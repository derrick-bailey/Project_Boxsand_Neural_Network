{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import mixed_precision\n",
    "import tensorboard\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "#Clear session and set random seeds to static\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def import_data(): \n",
    "\n",
    "    #week 11\n",
    "    path = r'C:\\Users\\Derrick\\Box\\Anon Data\\Undergrad Students\\Derrick Bailey\\NN Data\\week11'\n",
    "    file = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "    onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "    dfs = []\n",
    "    for f in file:\n",
    "        df = pd.read_csv(f)\n",
    "        dfs.append(df)\n",
    "    dfs = np.array(dfs, dtype=object)\n",
    "\n",
    "    institutional_Data = dfs[0]\n",
    "\n",
    "    gradebook = dfs[1]\n",
    "\n",
    "    #drop students who do have missing exam scores\n",
    "    gradebook['M1%'].replace('', 0, inplace=True)\n",
    "    gradebook['M2%'].replace('', 0, inplace=True)\n",
    "    gradebook['Final%'].replace('', 0, inplace=True)\n",
    "\n",
    "    gradebook = gradebook.dropna(axis=0, how='all')\n",
    "\n",
    "    gradebook = gradebook[gradebook['M1%'] != 0]\n",
    "    gradebook = gradebook[gradebook['M2%'] != 0]\n",
    "    gradebook = gradebook[gradebook['Final%'] != 0]\n",
    "\n",
    "    # with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    #     display(gradebook[['M1%','M2%','Final%']])\n",
    "\n",
    "    clickstream1 = dfs[2] \n",
    "    clickstream2 = dfs[3]\n",
    "\n",
    "    headers = list(clickstream1)\n",
    "    clickstream2.columns = headers\n",
    "    clickstream = pd.concat([clickstream1, clickstream2], ignore_index=False)\n",
    "\n",
    "    institutional_Data_Students = institutional_Data['ID'].unique()\n",
    "    institutional_Data_Students = pd.DataFrame(institutional_Data_Students)\n",
    "    institutional_Data_Students = institutional_Data_Students.dropna(axis=0, how='all')\n",
    "    institutional_Data_Students.columns = ['ID']\n",
    "\n",
    "    gradebook_Students = gradebook['ID'].unique()\n",
    "    gradebook_Students = pd.DataFrame(gradebook_Students)\n",
    "    gradebook_Students = gradebook_Students.dropna(axis=0, how='all')\n",
    "    gradebook_Students.columns = ['ID']\n",
    "\n",
    "    clickstream_Students = clickstream['ID'].unique()\n",
    "    clickstream_Students = pd.DataFrame(clickstream_Students)\n",
    "    clickstream_Students = clickstream_Students.dropna(axis=0, how='all')\n",
    "    clickstream_Students.columns = ['ID']\n",
    "\n",
    "    #delete students in clickstream and institutional if not present in gradebook\n",
    "    institutional_Data_Students = institutional_Data_Students[institutional_Data_Students.ID.isin(gradebook_Students.ID)].dropna()\n",
    "    clickstream_Students = clickstream_Students[clickstream_Students.ID.isin(gradebook_Students.ID)].dropna()\n",
    "\n",
    "    #check that students in institutional_Data match students in clickstream\n",
    "    class StudentError(Exception):\n",
    "        pass\n",
    "    try:\n",
    "        student_Check = clickstream_Students.iloc[:,0].str.strip().str.lower().unique().all() == institutional_Data_Students.iloc[:,0].str.strip().str.lower().unique().all() == gradebook_Students.iloc[:,0].str.strip().str.lower().unique().all()\n",
    "        if student_Check != True:\n",
    "            raise StudentError\n",
    "    except StudentError:\n",
    "        print(\"Students in Dataframes do not match!\")\n",
    "        assert False\n",
    "    except(SyntaxError):\n",
    "        print(\"Syntax Issue!\")\n",
    "        assert False\n",
    "\n",
    "    #Collect demographics\n",
    "#     assert demographic\n",
    "    demo_gate = input(\"Use demographics? Y/N: \")\n",
    "    if demo_gate == \"Y\":\n",
    "        demo = input(\"Select Demographic. Type EXACTLY as contained in the .csv file: \")\n",
    "#         demo = \"W - White\"\n",
    "        demo_type = input(\"Column name of demographic. Type EXACTLY as contained in the .csv file: \")\n",
    "        institutional_Data = institutional_Data[institutional_Data[demo_type].str.contains(demo)==True]\n",
    "\n",
    "#     display(institutional_Data)\n",
    "\n",
    "    #Parse through Institutional Data to generate featureset\n",
    "    institutional_Data_cols = list(institutional_Data.columns)\n",
    "    # drop any and all columns that are all ZERO (necessary: columns of zeroes will influence weighting and prediction)\n",
    "    institutional_Data = institutional_Data.loc[:, (institutional_Data !=0).any(axis=0)]\n",
    "    # drop any and all columns that are all NaN (necessary step - matrix operation on NaN will result in all operations NaN)\n",
    "    institutional_Data = institutional_Data.dropna(axis=1, how='all')\n",
    "    # fill NaN with zeroes\n",
    "    institutional_Data = institutional_Data.fillna(0)\n",
    "    institutional_Data = institutional_Data[institutional_Data.ID.isin(institutional_Data_Students.ID)]\n",
    "    institutional_Data_Features = [\"ID\",\"OVERALL OSU GPA\",\"OSU GPA\",\"OSU CREDITS ATTEMPT\",\"OSU CREDITS EARNED\"]\n",
    "    institutional_Data = institutional_Data.loc[:, institutional_Data_Features].copy()\n",
    "\n",
    "    # display(gradebook_Students)\n",
    "\n",
    "    #delete students in clickstream and gradebook if not present in institutional_data (demographic filter)\n",
    "    institutional_Data_Students = institutional_Data_Students[institutional_Data_Students.ID.isin(institutional_Data.ID)].dropna()\n",
    "    clickstream_Students = clickstream_Students[clickstream_Students.ID.isin(institutional_Data.ID)].dropna()\n",
    "    gradebook_Students = gradebook_Students[gradebook_Students.ID.isin(institutional_Data.ID)].dropna()\n",
    "\n",
    "    # display(gradebook_Students)\n",
    "\n",
    "    institutional_Data = institutional_Data[institutional_Data.ID.isin(institutional_Data_Students.ID)].dropna()\n",
    "    clickstream = clickstream[clickstream.ID.isin(clickstream_Students.ID)].dropna()\n",
    "    gradebook = gradebook[gradebook.ID.isin(gradebook_Students.ID)]\n",
    "\n",
    "    # display(institutional_Data)\n",
    "    # display(clickstream)\n",
    "    # display(gradebook[gradebook.ID.isin(gradebook_Students.ID)])\n",
    "\n",
    "    #recatch students\n",
    "    institutional_Data_Students = institutional_Data['ID'].unique()\n",
    "    institutional_Data_Students = pd.DataFrame(institutional_Data_Students)\n",
    "    institutional_Data_Students = institutional_Data_Students.dropna(axis=0, how='all')\n",
    "    institutional_Data_Students.columns = ['ID']\n",
    "\n",
    "    gradebook_Students = gradebook['ID'].unique()\n",
    "    gradebook_Students = pd.DataFrame(gradebook_Students)\n",
    "    gradebook_Students = gradebook_Students.dropna(axis=0, how='all')\n",
    "    gradebook_Students.columns = ['ID']\n",
    "\n",
    "    clickstream_Students = clickstream['ID'].unique()\n",
    "    clickstream_Students = pd.DataFrame(clickstream_Students)\n",
    "    clickstream_Students = clickstream_Students.dropna(axis=0, how='all')\n",
    "    clickstream_Students.columns = ['ID']\n",
    "\n",
    "    #Parse through Gradebook Data to generate featureset\n",
    "    gradebook_cols = list(gradebook.columns)\n",
    "    #feature list for gradebook\n",
    "    gradebook_features = ['ID', 'raw','online','HW%','RW%','Rec %','Lab %','M1%','M2%','Final%','Overall Grade %']\n",
    "    gradebook = gradebook[gradebook_features]\n",
    "    del gradebook['M1%']\n",
    "    del gradebook['M2%']\n",
    "    del gradebook['Final%']\n",
    "    gradebook = gradebook[gradebook.ID.isin(institutional_Data.ID)].dropna()\n",
    "    final_Grade = gradebook['Overall Grade %']\n",
    "    final_Grade = pd.DataFrame([final_Grade]).T\n",
    "    final_Grade = final_Grade.values\n",
    "    final_Grade = pd.DataFrame(final_Grade)\n",
    "    # remove final grade from gradebook\n",
    "    del gradebook['Overall Grade %']\n",
    "    # drop any and all columns that are all ZERO (necessary: columns of zeroes will influence weighting and prediction)\n",
    "    gradebook = gradebook.loc[:, (gradebook !=0).any(axis=0)]\n",
    "    # drop any and all columns that are all NaN (necessary step - matrix operation on NaN will result in all operations NaN)\n",
    "    gradebook = gradebook.dropna(axis=1, how='all')\n",
    "    # fill NaN with zeroes\n",
    "    gradebook = gradebook.fillna(0)\n",
    "    # recatch cols var\n",
    "    cols = list(gradebook.columns)\n",
    "    gradebook = gradebook[gradebook.ID.isin(gradebook_Students.ID)]\n",
    "\n",
    "    #Parse through Clickstream Data to generate featureset\n",
    "    clickstream_cols = list(clickstream.columns)\n",
    "    clickstream.set_index=clickstream['ID']\n",
    "    # drop any and all columns that are all ZERO (necessary: columns of zeroes will influence weighting and prediction)\n",
    "    clickstream = clickstream.loc[:, (clickstream !=0).any(axis=0)]\n",
    "    # drop any and all columns that are all NaN (necessary step - matrix operation on NaN will result in all operations NaN)\n",
    "    clickstream = clickstream.dropna(axis=1, how='all')\n",
    "    # fill NaN with zeroes\n",
    "    clickstream = clickstream.fillna(0)\n",
    "    clickstream = clickstream[clickstream.ID.isin(clickstream_Students.ID)]\n",
    "\n",
    "    # now to parse through for specific features\n",
    "    # create list out of student column\n",
    "    student_list = clickstream[['ID']]\n",
    "    student_list_unique = pd.DataFrame(student_list['ID'].unique())\n",
    "    student_list_unique.columns = ['ID']\n",
    "    clickstream_features = ['m1_sols', 'm2_sols','f_sols', '#Practice', '#Fundamental', 'Calendar', 'KALTURA', 'youtube']\n",
    "    clickstream_cut = clickstream[clickstream['Page title'].str.contains(\"|\".join(clickstream_features))]\n",
    "\n",
    "    studentdf = []\n",
    "    studentdf = [x for _, x in clickstream_cut.groupby(by='ID')]\n",
    "\n",
    "    clickstream_Data = []\n",
    "    clickstream_Data = pd.DataFrame(columns = clickstream_features)\n",
    "    clickstream_Data = student_list_unique.append(clickstream_Data)\n",
    "\n",
    "    count = []\n",
    "\n",
    "    #automated feature-by-feature approach\n",
    "    for feature in clickstream_features:\n",
    "        for pos in studentdf:\n",
    "            for ID in pos['ID'].unique():\n",
    "                ID = pos['ID'].unique()\n",
    "                count.append(pos['Page title'].str.contains(feature).sum())\n",
    "        clickstream_Data[feature] = count\n",
    "        count.clear()\n",
    "\n",
    "    frames = pd.merge(institutional_Data, gradebook, on='ID')\n",
    "    frames = pd.merge(frames, clickstream_Data, on='ID')\n",
    "\n",
    "    #     final_Grade = final_Grade[final_Grade.ID.isin(institutional_Data_Students.ID)].dropna()\n",
    "\n",
    "    input_Features = frames\n",
    "    feature_num = len(input_Features.columns)\n",
    "\n",
    "    #week 4\n",
    "    pre_Lecture_Max = {\n",
    "        \"AQ.L1.1\":8,\n",
    "        \"AQ.L2.1\":8,\n",
    "        \"GA.L1.1\":6,\n",
    "        \"GR.L1.1\":18,\n",
    "        \"K1.L1.1\":6,\n",
    "        \"K1.L2.1\":6,\n",
    "        \"K2.L1.1\":6,\n",
    "        \"K2.L2.1\":6,\n",
    "        \"VO.L1.1\":11\n",
    "    }\n",
    "    pre_Lecture_Max_Values = pre_Lecture_Max.values()\n",
    "    pre_Lecture_Max_Values_List = list(pre_Lecture_Max_Values)\n",
    "    pre_Lecture_Keys = list(pre_Lecture_Max.keys())\n",
    "\n",
    "    post_Lecture_Max = {\n",
    "        \"AQ.L1.3\":5,\n",
    "        \"AQ.L2.3\":4,\n",
    "        \"GA.L1.3\":8,\n",
    "        \"GR.L1.3\":13,\n",
    "        \"K1.L1.3\":8,\n",
    "        \"K1.L2.3\":9,\n",
    "        \"K2.L1.3\":4,\n",
    "        \"K2.L2.3\":15,\n",
    "        \"VO.L1.3\":16\n",
    "    }\n",
    "    post_Lecture_Max_Values = post_Lecture_Max.values()\n",
    "    post_Lecture_Max_Values_List = list(post_Lecture_Max_Values)\n",
    "    post_Lecture_Keys = list(post_Lecture_Max.keys())\n",
    "\n",
    "    path = r'C:\\Users\\Derrick\\Box\\Anon Data\\Undergrad Students\\Derrick Bailey\\NN Data\\week4'\n",
    "    file = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "    onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "    pre_Lecture = []\n",
    "    post_Lecture = []\n",
    "\n",
    "    for f in file:\n",
    "        with open(f) as src_file:\n",
    "            encoding = src_file.encoding\n",
    "        if 'pre' in f.lower():\n",
    "    #         print(\"Test1\")\n",
    "            df = pd.read_csv(f, encoding=encoding, usecols=['sis_id','attempt','score'])\n",
    "            pre_Lecture.append(df)\n",
    "        if 'post' in f.lower():\n",
    "    #         print(\"Test2\")\n",
    "            df = pd.read_csv(f, encoding=encoding, usecols=['sis_id','attempt','score'])\n",
    "            post_Lecture.append(df)\n",
    "\n",
    "    #grab students for pre and post lecture\n",
    "\n",
    "    prelec_students = []\n",
    "    postlec_students = []\n",
    "\n",
    "    for i in range(len(pre_Lecture)):\n",
    "        holder = pd.DataFrame(pre_Lecture[i])\n",
    "        holder['sis_id'].unique()\n",
    "        for string in holder['sis_id']:\n",
    "            if string not in prelec_students:\n",
    "                prelec_students.append(string)\n",
    "    prelec_students = pd.DataFrame(prelec_students)\n",
    "    prelec_students.rename(columns={0:'ID'}, inplace=True)\n",
    "\n",
    "    for i in range(len(post_Lecture)):\n",
    "        holder = pd.DataFrame(post_Lecture[i])\n",
    "        holder['sis_id'].unique()\n",
    "        for string in holder['sis_id']:\n",
    "            if string not in postlec_students:\n",
    "                postlec_students.append(string)\n",
    "    postlec_students = pd.DataFrame(postlec_students)\n",
    "    postlec_students.rename(columns={0:'ID'}, inplace=True)\n",
    "\n",
    "    #compare students from assignments to gradebook students, delete those not present in gradebook\n",
    "\n",
    "    prelec_students = prelec_students[prelec_students.ID.isin(institutional_Data.ID)].dropna()\n",
    "    postlec_students = postlec_students[postlec_students.ID.isin(institutional_Data.ID)].dropna()\n",
    "\n",
    "    pre_Lecture = np.array(pre_Lecture, dtype=object)\n",
    "    post_Lecture = np.array(post_Lecture, dtype=object)\n",
    "\n",
    "    #now we have an array of arrays containing unique students for every pre/post lecture, and two array of arrays containing the pre/post lectures. \n",
    "    #Need to get maximum score for every student and get attempt num\n",
    "\n",
    "    popout_prelec = pre_Lecture[0]\n",
    "    popout_prelec = pd.DataFrame(popout_prelec)\n",
    "    popout_postlec = post_Lecture[0]\n",
    "    popout_postlec = pd.DataFrame(popout_postlec)\n",
    "\n",
    "    #go through students, find max score, keep row, drop other rows containing studentxxx\n",
    "    pre_Lecture_modified = []\n",
    "    post_Lecture_modified = []\n",
    "\n",
    "    i=0\n",
    "    for array in post_Lecture:\n",
    "    #initial dataframe\n",
    "        popout_postlec = array\n",
    "        df_max = popout_postlec.groupby('sis_id').idxmax()\n",
    "        df_max['type'] = 'max'\n",
    "    #index sort\n",
    "        df2 = df_max.set_index('type',append=True).stack().rename('index')\n",
    "    #concatenation\n",
    "        df3 = pd.concat([ df2.reset_index().drop('sis_id',axis=1).set_index('index'), \n",
    "                          popout_postlec.loc[df2.values] ], axis=1)\n",
    "        df3.set_index(['sis_id','level_2','type']).sort_index()\n",
    "    #groupby aggregate and reset index   \n",
    "        df3 = df3.groupby(['sis_id', 'attempt']).agg({'score': 'max'}).reset_index()\n",
    "        df3 = df3.drop_duplicates(subset=['sis_id'], keep='first').reset_index(drop=True)\n",
    "        df3['Percent'] = df3['score'] / post_Lecture_Max_Values_List[i]\n",
    "        i = i + 1\n",
    "        post_Lecture_modified.append(df3)\n",
    "\n",
    "    # print(\"PRE LEC \\n \\n \\n\")    \n",
    "\n",
    "    j=0\n",
    "    for array in pre_Lecture:\n",
    "    #initial dataframe\n",
    "        popout_prelec = array\n",
    "    #groupby ID\n",
    "        df_max = popout_prelec.groupby('sis_id').idxmax()\n",
    "        df_max['type'] = 'max'\n",
    "    #index sort\n",
    "        df2 = df_max.set_index('type',append=True).stack().rename('index')\n",
    "    #concatenation\n",
    "        df3 = pd.concat([ df2.reset_index().drop('sis_id',axis=1).set_index('index'), \n",
    "                          popout_prelec.loc[df2.values] ], axis=1)\n",
    "        df3.set_index(['sis_id','level_2','type']).sort_index()\n",
    "    #groupby aggregate and reset index    \n",
    "        df3 = df3.groupby(['sis_id', 'attempt']).agg({'score': 'max'}).reset_index()\n",
    "        df3 = df3.drop_duplicates(subset=['sis_id'], keep='first').reset_index(drop=True)\n",
    "        df3['Percent'] = df3['score'] / pre_Lecture_Max_Values_List[j]\n",
    "        j = j + 1\n",
    "        pre_Lecture_modified.append(df3)\n",
    "\n",
    "    # print(\"-------------------------------------\", \"\\n\", \"PRELECTURE\", \"\\n\", \"-------------------------------------\")\n",
    "    number = 0\n",
    "    pre_Lecture_finished = []\n",
    "    for frame in pre_Lecture_modified:\n",
    "    #if student ID not in assignment but in overall prelecture submissions, add student ID\n",
    "        for student in list(prelec_students['ID'].sort_values()):\n",
    "    #         print(student)\n",
    "            if student not in list(frame['sis_id'].unique()):\n",
    "    #             print(\"Missing: \", student)\n",
    "                frame.loc[len(frame.index) + 1] = student\n",
    "        frame['attempt'] = pd.to_numeric(frame['attempt'], errors='coerce').fillna(0)\n",
    "        frame['score'] = pd.to_numeric(frame['score'], errors='coerce').fillna(0)\n",
    "        frame['Percent'] = pd.to_numeric(frame['Percent'], errors='coerce').fillna(0)\n",
    "        frame.rename(columns={\"attempt\": \"prelec_Attempt\", \"score\": \"prelec_Score\", \"Percent\": \"prelec_Percent\"}, inplace=True)\n",
    "    #if student ID in assignment but did not finish course, delete student ID\n",
    "        for student in list(frame['sis_id'].unique()):\n",
    "    #         print(\"Student from frame: \", student)\n",
    "            if student not in list(prelec_students['ID'].sort_values()):\n",
    "    #             print(\"Extraneous: \", student)\n",
    "                frame = frame[frame[\"sis_id\"].str.contains(student)==False]\n",
    "        number += 1\n",
    "        pre_Lecture_finished.append(frame.sort_values('sis_id'))\n",
    "\n",
    "    del number    \n",
    "    del frame\n",
    "\n",
    "    # display(pre_Lecture_modified)\n",
    "    # print(\"-------------------------------------\", \"\\n\", \"POSTLECTURE\", \"\\n\", \"-------------------------------------\")\n",
    "\n",
    "    number = 0\n",
    "    post_Lecture_finished = []\n",
    "    for frame in post_Lecture_modified:\n",
    "    #if student ID not in assignment but in overall postlecture submissions, add student ID\n",
    "        for student in list(postlec_students['ID'].sort_values()):\n",
    "    #         print(student)\n",
    "            if student not in list(frame['sis_id'].unique()):\n",
    "    #             print(\"Missing: \", student)\n",
    "                frame.loc[len(frame.index) + 1] = student\n",
    "        frame['attempt'] = pd.to_numeric(frame['attempt'], errors='coerce').fillna(0)\n",
    "        frame['score'] = pd.to_numeric(frame['score'], errors='coerce').fillna(0)\n",
    "        frame['Percent'] = pd.to_numeric(frame['Percent'], errors='coerce').fillna(0)\n",
    "        frame.rename(columns={\"attempt\": \"postlec_Attempt\", \"score\": \"postlec_Score\", \"Percent\": \"postlec_Percent\"}, inplace=True)\n",
    "    #if student ID in assignment but did not finish course, delete student ID\n",
    "        for student in list(frame['sis_id'].unique()):\n",
    "    #         print(\"Student from frame: \", student)\n",
    "            if student not in list(postlec_students['ID'].sort_values()):\n",
    "    #             print(\"Extraneous: \", student)\n",
    "                frame = frame[frame[\"sis_id\"].str.contains(student)==False]\n",
    "        number += 1\n",
    "        post_Lecture_finished.append(frame.sort_values('sis_id'))\n",
    "    # for frame in post_Lecture_modified:\n",
    "    #     display(frame)\n",
    "    #     frame.to_csv(r'C:\\Users\\Derrick\\Box\\Anon Data\\Undergrad Students\\Derrick Bailey\\NN Data\\week4\\frame_%s.csv' %number)\n",
    "\n",
    "    del frame\n",
    "    del number\n",
    "\n",
    "    assignment = 0\n",
    "    for item in pre_Lecture_finished:\n",
    "        item.drop(['prelec_Score'], axis=1, inplace=True)\n",
    "        item.rename(columns={'sis_id':'ID'}, inplace=True)\n",
    "        item.rename(columns={'prelec_Attempt': pre_Lecture_Keys[assignment] + ' Attempts', 'prelec_Percent': pre_Lecture_Keys[assignment] + ' Percent'}, inplace=True)\n",
    "        assignment += 1\n",
    "\n",
    "    del assignment\n",
    "\n",
    "    assignment = 0\n",
    "    for item in post_Lecture_finished:\n",
    "        item.drop(['postlec_Score'], axis=1, inplace=True)\n",
    "        item.rename(columns={'sis_id':'ID'}, inplace=True)\n",
    "        item.rename(columns={'postlec_Attempt': post_Lecture_Keys[assignment] + ' Attempts', 'postlec_Percent': post_Lecture_Keys[assignment] + ' Percent'}, inplace=True)\n",
    "        assignment += 1\n",
    "\n",
    "    del assignment\n",
    "\n",
    "    pre_and_post = pre_Lecture_finished + post_Lecture_finished\n",
    "    \n",
    "    test_data = reduce(lambda x, y: pd.merge(x, y, on = 'ID'), pre_and_post)\n",
    "\n",
    "#     input_Features.drop(columns=['m1_sols','m2_sols','f_sols', 'Lab %'], inplace=True)\n",
    "\n",
    "#     with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#         display(input_Features)\n",
    "\n",
    "    train_data = input_Features\n",
    "\n",
    "    return train_data, test_data, final_Grade, studentdf, student_list_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def debug(debug=False):\n",
    "    while debug == True:\n",
    "        debug_input = int(input(\"Invoke the rite of passage! \\n \\\n",
    "        1: Final Grades Dataframe \\n \\\n",
    "        2: Student Dataframe \\n \\\n",
    "        3: Input Features Dataframe \\n \\\n",
    "        4: Exit \\n\"))\n",
    "        if debug_input == 1:\n",
    "            print(\"Final Grades Dataframe\")\n",
    "            display(final_Grade)\n",
    "            break\n",
    "        elif debug_input == 2:\n",
    "            print(\"Student Dataframe\")\n",
    "            display(studentdf)\n",
    "            break\n",
    "        elif debug_input == 3:\n",
    "            print(\"Input Features Dataframe\")\n",
    "            display(input_Features)\n",
    "            break\n",
    "        elif debug_input == 4:\n",
    "            debug_input = False\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "def process_data(train_data, test_data, final_Grade):\n",
    "    # Split data for train/test\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(input_Features, final_Grade, \n",
    "#                                                         test_size=0.33, random_state=0)\n",
    "    # test\n",
    "#     x_train = np.array(x_train)\n",
    "#     x_test = np.array(x_test)\n",
    "#     y_train = np.array(y_train)\n",
    "#     y_test = np.array(y_test)\n",
    "    \n",
    "    final_Grade.columns = [\"final_Grade\"]\n",
    "    \n",
    "    x_train = train_data.join(final_Grade)\n",
    "    x_test = test_data.join(final_Grade)\n",
    "    \n",
    "#     y_train = final_Grade.sample(frac=0.67).sort_index()\n",
    "#     y_test = pd.concat([final_Grade, y_train]).drop_duplicates(keep=False).sort_index()\n",
    "    \n",
    "#     display(\"x_train\", x_train)\n",
    "#     display(\"x_test\", x_test)\n",
    "    \n",
    "    x_train_student = pd.DataFrame(x_train['ID'])\n",
    "    x_test_student = pd.DataFrame(x_test['ID'])\n",
    "    \n",
    "#     display(\"x_train_student\", x_train_student)\n",
    "#     display(\"x_test_student\", x_test_student)\n",
    "    \n",
    "    y_train = x_train['final_Grade'].sample(frac=0.67).sort_index()\n",
    "    \n",
    "#     display(\"y_train\", y_train)\n",
    "#     display(y_train.index)\n",
    "\n",
    "    x_train_index = list(x_train.index)\n",
    "    y_train_index = list(y_train.index)\n",
    "    train_index_diff = [elem for elem in x_train_index if elem not in y_train_index]\n",
    "    \n",
    "    y_test = x_train.drop(index=y_train.index, inplace=False)\n",
    "    y_test = y_test['final_Grade']\n",
    "    \n",
    "    x_train = x_train.drop(index = train_index_diff, inplace=False)\n",
    "    \n",
    "    x_test_index = list(x_test.index)\n",
    "    y_test_index = list(y_test.index)\n",
    "    test_index_diff = [elem for elem in x_test_index if elem not in y_test_index]\n",
    "    \n",
    "    x_test = x_test.drop(index = test_index_diff, inplace=False)\n",
    "    \n",
    "    \n",
    "#     display(\"y_test\", y_test)\n",
    "\n",
    "#     display(\"x_train\", x_train.shape)\n",
    "#     display(\"x_test\", x_test.shape)\n",
    "#     display(\"y_train\", y_train.shape)\n",
    "#     display(\"y_test\", y_test.shape)\n",
    "\n",
    "#     display(\"x_train\", x_train)\n",
    "#     display(\"x_test\", x_test)\n",
    "#     display(\"y_train\", y_train)\n",
    "#     display(\"y_test\", y_test)\n",
    "    \n",
    "    x_train = np.array(x_train)\n",
    "    x_test = np.array(x_test)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    y_train = np.reshape(y_train, (len(y_train), 1))\n",
    "    y_test = np.reshape(y_test, (len(y_test), 1))\n",
    "    \n",
    "    display(\"x_train\", x_train.shape)\n",
    "    display(\"x_test\", x_test.shape)\n",
    "    display(\"y_train\", y_train.shape)\n",
    "    display(\"y_test\", y_test.shape)\n",
    "\n",
    "    # delete student column before normalization but saving student column\n",
    "    x_train_student = x_train[:,0]\n",
    "    x_test_student = x_test[:,0]\n",
    "    x_train = np.delete(x_train, 0, 1)\n",
    "    x_test = np.delete(x_test, 0, 1)\n",
    "    \n",
    "#     display(\"x_train\", x_train)\n",
    "#     display(\"x_test\", x_test)\n",
    "#     display(\"y_train\", y_train)\n",
    "#     display(\"y_test\", y_test)\n",
    "    \n",
    "    # Normalization\n",
    "    sc = MinMaxScaler(feature_range = (0,1))\n",
    "    x_train_norm = sc.fit_transform(x_train)\n",
    "    x_test_norm = sc.fit_transform(x_test)\n",
    "    y_train_norm = sc.fit_transform(y_train)\n",
    "    y_test_norm = sc.fit_transform(y_test)\n",
    "\n",
    "    x_train_norm = np.reshape(x_train_norm, (x_train_norm.shape[0], x_train_norm.shape[1], 1))\n",
    "    x_test_norm = np.reshape(x_test_norm, (x_test_norm.shape[0], x_test_norm.shape[1], 1))\n",
    "    \n",
    "    return x_train_norm, x_test_norm, y_train_norm, y_test_norm, x_train_student, x_test_student, sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def student_cross_ref(student_list_unique, final_Grade, predicted_Grade):\n",
    "    # We must cross-reference the split data with the final grade master list\n",
    "    # and pull only the matching student numbers from the final grade master list.\n",
    "\n",
    "    # Start by zipping together student dataframe and final grade dataframe.\n",
    "    final_Grade = np.hstack((student_list_unique, final_Grade))\n",
    "    final_Grade = pd.DataFrame(final_Grade)\n",
    "    # To make our lives easy, let's name the columns for our predicted and final grade dataframes.\n",
    "    final_Grade.columns = ['FStudent','FGrade']\n",
    "    predicted_Grade.columns = ['PStudent','PGrade']\n",
    "    # Now, we compare the student columns for these dataframes.\n",
    "    merged_Grades = final_Grade.merge(predicted_Grade, left_on = 'FStudent', right_on = 'PStudent', how = 'left')\n",
    "    # with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    #     display(merged_Grades)\n",
    "    merged_Grades = merged_Grades[merged_Grades['PStudent'].notna()]\n",
    "    return merged_Grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_optimization(hyp_opt):   \n",
    "    if hyp_opt == \"Y\":\n",
    "        #this is way too big.\n",
    "#         param_list = [[*range(100,1100,100)], \n",
    "#                       [*map(lambda x: 2 ** x, range(8,-1,-1))], \n",
    "#                       [*map(lambda x: 1*10**(-x), range(5))], \n",
    "# #                       [*map(lambda x: 1*10**(-1-x), range(5))], \n",
    "#                       list(np.linspace(0.05,0.3,6).round(decimals=2)),\n",
    "#                       list(np.linspace(0.05,0.3,6).round(decimals=2)), \n",
    "#                       list(np.linspace(0.05,0.3,6).round(decimals=2)), \n",
    "#                       list(np.linspace(0.05,0.3,6).round(decimals=2)), \n",
    "#                       list(np.linspace(0.05,0.3,6).round(decimals=2)), \n",
    "#                       list(np.linspace(0.05,0.3,6).round(decimals=2)),\n",
    "#                       [*map(lambda x: 2 ** x, range(10))],\n",
    "#                       [*map(lambda x: 2 ** x, range(10))], \n",
    "#                       [*map(lambda x: 2 ** x, range(10))]]\n",
    "        param_list = [[500], \n",
    "                      [1,8], \n",
    "                      [1e-4],\n",
    "                      [0.15],\n",
    "                      [0.15], \n",
    "                      [0.15], \n",
    "                      [0.2], \n",
    "                      [0.2], \n",
    "                      [0.2],\n",
    "                      [*map(lambda x: 5 ** x, range(5,7))],\n",
    "                      [*map(lambda x: 5 ** x, range(4,6))], \n",
    "                      [*map(lambda x: 5 ** x, range(3,6))]]\n",
    "        hype_var_list_index = ['num_epochs', 'batch_size', 'lr', 'recurrent_dropout1', 'recurrent_dropout2', 'recurrent_dropout3', 'dropout1', 'dropout2', 'dropout3', 'gru_units1', 'gru_units2', 'gru_units3']\n",
    "\n",
    "        num_epochs, batch_size, lr, recurrent_dropout1, recurrent_dropout2, recurrent_dropout3, dropout1, dropout2, dropout3, gru_units1, gru_units2, gru_units3 = [param_list[i] for i in range(len(param_list))]\n",
    "        hype_var_list = pd.DataFrame(index=hype_var_list_index,\n",
    "                                      data=([num_epochs], \n",
    "                                      [batch_size], \n",
    "                                      [lr], \n",
    "                                      [recurrent_dropout1], \n",
    "                                      [recurrent_dropout2], \n",
    "                                      [recurrent_dropout3], \n",
    "                                      [dropout1], \n",
    "                                      [dropout2], \n",
    "                                      [dropout3], \n",
    "                                      [gru_units1], \n",
    "                                      [gru_units2], \n",
    "                                      [gru_units3]))\n",
    "        optimize_gate = True\n",
    "#         hype_var_list.DataFrame()\n",
    "#         print(hype_var_list)\n",
    "#         print(hype_var_list)\n",
    "        return hype_var_list, optimize_gate\n",
    "    if hyp_opt == \"N\":\n",
    "        param_list = [500, 1, 1e-4, 0.15, 0.15, 0.15, 0.2, 0.2, 0.2, 625, 125, 5]\n",
    "        num_epochs, batch_size, lr, recurrent_dropout1, recurrent_dropout2, recurrent_dropout3, dropout1, dropout2, dropout3, gru_units1, gru_units2, gru_units3 = [param_list[i] for i in range(len(param_list))]\n",
    "        hype_var_list = [num_epochs, \n",
    "                         batch_size, \n",
    "                         lr, \n",
    "                         recurrent_dropout1, \n",
    "                         recurrent_dropout2, \n",
    "                         recurrent_dropout3, \n",
    "                         dropout1, \n",
    "                         dropout2, \n",
    "                         dropout3, \n",
    "                         gru_units1, \n",
    "                         gru_units2, \n",
    "                         gru_units3]\n",
    "        optimize_gate = False\n",
    "        return hype_var_list, optimize_gate\n",
    "    elif hyp_opt != \"Y\" and hyp_opt != \"N\":\n",
    "        print(\"Try Again!\")\n",
    "        hyp_opt = hyperparameter_optimization(input(\"Optimize? \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateModel(hype_var_list, optimize_gate, x_train_norm, y_train_norm, x_test_norm, x_test_student, sc, student_list_unique, final_Grade):\n",
    "    if optimize_gate == True:\n",
    "        index = hype_var_list.index\n",
    "        for i in hype_var_list:\n",
    "            num_epochs, batch_size, lr, recurrent_dropout1, recurrent_dropout2, recurrent_dropout3, dropout1, dropout2, dropout3, gru_units1, gru_units2, gru_units3 = hype_var_list[i]\n",
    "        for a in range(len(num_epochs)):\n",
    "#             print(\"num_epochs: \", a, \"\\n\")\n",
    "            for b in range(len(batch_size)):\n",
    "#                 print(\"batch_size: \", b, \"\\n\")\n",
    "                for c in range(len(lr)):\n",
    "#                     print(\"lr: \", c, \"\\n\")\n",
    "#                         print(\"decay: \", d, \"\\n\")\n",
    "                        for d in range(len(recurrent_dropout1)):\n",
    "#                             print(\"recurrent_dropout1: \", e, \"\\n\")\n",
    "                            for e in range(len(recurrent_dropout2)):\n",
    "#                                 print(\"recurrent_dropout2: \", f, \"\\n\")\n",
    "                                for f in range(len(recurrent_dropout3)):\n",
    "#                                     print(\"recurrent_dropout3: \", g, \"\\n\")\n",
    "                                    for g in range(len(dropout1)):\n",
    "#                                         print(\"dropout1: \", h, \"\\n\")\n",
    "                                        for h in range(len(dropout2)):\n",
    "#                                             print(\"dropout2: \", i, \"\\n\")\n",
    "                                            for i in range(len(dropout3)):\n",
    "#                                                 print(\"dropout3: \", j, \"\\n\")\n",
    "                                                for j in range(len(gru_units1)):\n",
    "#                                                     print(\"gru_units1: \", k, \"\\n\")\n",
    "                                                    for k in range(len(gru_units2)):\n",
    "#                                                         print(\"gru_units2: \", l, \"\\n\")\n",
    "                                                        for l in range(len(gru_units3)):\n",
    "#                                                             print(\"gru_units3: \", m, \"\\n\")\n",
    "                                                            hype_list = [num_epochs[a],batch_size[b],lr[c],recurrent_dropout1[d],recurrent_dropout2[e],recurrent_dropout3[f],dropout1[g],dropout2[h],dropout3[i],gru_units1[j],gru_units2[k],gru_units3[l]]\n",
    "#                                                             print(hype_list)\n",
    "                                                            BuildModel(x_train_norm, y_train_norm, x_test_norm, x_test_student, sc, hype_list, student_list_unique, final_Grade)\n",
    "        #BuildModel()\n",
    "#         pass\n",
    "    if optimize_gate == False:\n",
    "        #train on defaults\n",
    "#         print(hype_var_list)\n",
    "#         for i in hype_var_list:\n",
    "        num_epochs, batch_size, lr, recurrent_dropout1, recurrent_dropout2, recurrent_dropout3, dropout1, dropout2, dropout3, gru_units1, gru_units2, gru_units3 = hype_var_list\n",
    "#         print(num_epochs, batch_size, lr, recurrent_dropout1, recurrent_dropout2, recurrent_dropout3, dropout1, dropout2, dropout3, gru_units1, gru_units2, gru_units3)\n",
    "        hype_list = [num_epochs, batch_size, lr, recurrent_dropout1, recurrent_dropout2, recurrent_dropout3, dropout1, dropout2, dropout3, gru_units1, gru_units2, gru_units3]\n",
    "        BuildModel(x_train_norm, y_train_norm, x_test_norm, x_test_student, sc, hype_list, student_list_unique, final_Grade)\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_chaos(merged_Grades, loss_df, mae, mse, fit, timestamp):\n",
    "    # Visualize the chaos\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2)\n",
    "    ax1.scatter(merged_Grades.iloc[:,3],merged_Grades.iloc[:,1], c=merged_Grades.iloc[:,3], cmap = 'jet_r', edgecolors='black')\n",
    "    ax1.set_xlabel('Predicted Grade')\n",
    "    ax1.set_ylabel('Actual Grade')\n",
    "    min_predicted = merged_Grades['PGrade'].min()\n",
    "    ax1.set_xlim(0,100)\n",
    "    ax1.set_ylim(0,100)\n",
    "\n",
    "    ax2.plot(loss_df[['loss', 'val_loss']])\n",
    "#     print(loss_df)\n",
    "    ax2.set_xlabel(\"Number of Epochs\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "    ax2.plot(fit.history['loss'], label = 'Loss on Training Set')\n",
    "    ax2.plot(fit.history['val_loss'], label = 'Loss on Validation Set')\n",
    "    ax2.legend()\n",
    "    textstr = '\\n'.join((\n",
    "        r'$MAE = %1.3f$' %mae,\n",
    "        r'$MSE = %1.3f$' %mse))\n",
    "\n",
    "    ax1.text(0.05, 0.95, textstr, transform = ax1.transAxes, fontsize = 14, va = 'top')\n",
    "\n",
    "    lims1 = [\n",
    "        np.min([ax1.get_xlim(), ax1.get_ylim()]),  # min of both axes\n",
    "        np.max([ax1.get_xlim(), ax1.get_ylim()]),  # max of both axes\n",
    "    ]\n",
    "\n",
    "    lims2 = [\n",
    "         np.min([ax2.get_xlim(), ax2.get_ylim()]),  # min of both axes\n",
    "         np.max([ax2.get_xlim(), ax2.get_ylim()]),  # max of both axes\n",
    "    ]\n",
    "\n",
    "    # now plot both limits against eachother\n",
    "    ax1.plot(lims1, lims1, 'k-', alpha=0.5, zorder=0)\n",
    "\n",
    "    ax1.grid()\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(right=2, wspace=0.2)\n",
    "    \n",
    "#     datetime = time.strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "#     fig.savefig(\"Figure \"+ datetime.str() + \".png\")\n",
    "    \n",
    "    plt.show()\n",
    "    figname = timestamp\n",
    "    fig.savefig(\"Graphs/GRU/Week 4/{}.svg\".format(figname), transparent=False, facecolor = 'w', bbox_inches='tight')\n",
    "    fig.savefig(\"Graphs/GRU/Week 4/{}.png\".format(figname), transparent=False, facecolor = 'w', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_heatmap(train):\n",
    "    train = pd.DataFrame(train, columns=cols)\n",
    "    correlations = train.corr()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    sns.heatmap(correlations, vmax=1.0, vmin=-1.0, center=0, fmt='.2f',\n",
    "                square=True, linewidths=1, annot=True, cmap = 'vlag', cbar_kws={\"shrink\": .70})\n",
    "    savegraph = input('Do you want to save this graph? Y/N: ')\n",
    "    if savegraph == \"Y\":\n",
    "        figname = input('Graph name: ')\n",
    "        fig.savefig(\"Graphs/{}.svg\".format(figname), transparent=False, facecolor = 'w', dpi=150)\n",
    "        fig.savefig(\"Graphs/{}.png\".format(figname), transparent=False, facecolor = 'w', dpi=150)\n",
    "    plt.show();\n",
    "# input_Features, final_Grade, studentdf, student_list_unique = import_data()\n",
    "# cols = list(input_Features.keys())\n",
    "# cols.pop(0)\n",
    "# x_train_norm, x_test_norm, y_train_norm, y_test_norm, x_train_student, x_test_student, sc = process_data(input_Features, final_Grade)\n",
    "# x_train_norm = pd.DataFrame(x_train_norm, columns=cols)\n",
    "# y_train_norm = pd.DataFrame(y_train_norm, columns=['final_Grade'])\n",
    "# combined = pd.concat([x_train_norm, y_train_norm], axis=1)\n",
    "# cols.append('final_Grade')\n",
    "# correlation_heatmap(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savegraph = input('Do you want to save this graph? Y/N: ')\n",
    "# if savegraph == \"Y\":\n",
    "#     figname = input('Graph name: ')\n",
    "#     fig.savefig(\"Graphs/{}.svg\".format(figname), transparent=False, facecolor = 'w', dpi=150)\n",
    "#     fig.savefig(\"Graphs/{}.png\".format(figname), transparent=False, facecolor = 'w', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_Students = input(\"Display students? Y/N: \")\n",
    "# display_Students.upper()\n",
    "# if display_Students == \"Y\":\n",
    "#     with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#         display(merged_Grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @tf.function(jit_compile=True)\n",
    "def BuildModel(x_train_norm, y_train_norm, x_test_norm, x_test_student, sc, hype_list, student_list_unique, final_Grade):\n",
    "    num_epochs, batch_size, lr, recurrent_dropout1, recurrent_dropout2, recurrent_dropout3, dropout1, dropout2, dropout3, gru_units1, gru_units2, gru_units3 = hype_list\n",
    "#     print(num_epochs, batch_size, lr, recurrent_dropout1, recurrent_dropout2, recurrent_dropout3, dropout1, dropout2, dropout3, gru_units1, gru_units2, gru_units3)\n",
    "#     print(x_train_norm.shape)\n",
    "\n",
    "    #GRU model\n",
    "#     type(x_train_norm)\n",
    "#     tf.keras.backend.set_floatx('float32')\n",
    "#     tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    \n",
    "    print(\"x_train_norm\")\n",
    "    print(x_train_norm.shape)\n",
    "    print(x_train_norm)\n",
    "    print(\"x_test_norm\")\n",
    "    print(x_test_norm.shape)\n",
    "    print(x_test_norm)\n",
    "    \n",
    "    policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.experimental.set_policy(policy)\n",
    "    display(hype_list)\n",
    "    with tf.device('/gpu:0'):\n",
    "        # NN Parameters\n",
    "        # Initialization\n",
    "        RNN = Sequential()\n",
    "        start = time.perf_counter()\n",
    "        # 1st GRU layer & Dropout regularisation\n",
    "        RNN.add(GRU(units=gru_units1, return_sequences=True, recurrent_dropout=recurrent_dropout1, input_shape=(x_train_norm.shape[1], 1)))\n",
    "        RNN.add(Dropout(dropout1))\n",
    "#         RNN.add(BatchNormalization())\n",
    "        # 2nd GRU Layer & Dropout regularisation\n",
    "        RNN.add(GRU(units=gru_units2, return_sequences=True, recurrent_dropout=recurrent_dropout2))\n",
    "        RNN.add(Dropout(dropout2))\n",
    "#         RNN.add(BatchNormalization())\n",
    "        # 3rd GRU Layer & Dropout regularisation\n",
    "        RNN.add(GRU(units=gru_units3, recurrent_dropout=recurrent_dropout3))\n",
    "        RNN.add(Dropout(dropout3))\n",
    "#         RNN.add(BatchNormalization())\n",
    "        # 4th GRU Layer\n",
    "#         RNN.add(GRU(units=50, recurrent_dropout=0.1))\n",
    "    #   RNN.add(BatchNormalization())\n",
    "        # Output Layer\n",
    "        RNN.add(Dense(units = 1))\n",
    "        # Compiling RNN\n",
    "        loss = tf.keras.losses.MeanSquaredError()\n",
    "        decayed_lr = tf.keras.optimizers.schedules.ExponentialDecay(lr, 1000, 0.96, staircase=False)\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=decayed_lr)\n",
    "        RNN.compile(optimizer=opt, loss=loss)\n",
    "\n",
    "        #Early Stop Callback\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "        min_delta = 1e-4, \n",
    "        patience=50, \n",
    "        restore_best_weights = True)\n",
    "        #Checkpoint Callback\n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=r'C:\\Users\\OSU_bailderr\\Box\\Anon Data\\Undergrad Students\\Derrick Bailey\\NN Repository\\Checkpoints',\n",
    "        save_weights_only=True,\n",
    "        monitor='val_loss',\n",
    "        mode='max',\n",
    "        save_best_only=True)\n",
    "        #Tensorboard Callback\n",
    "        logdir='logs\\\\fit\\\\' + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard = keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)\n",
    "\n",
    "        # Fitting RNN to training set\n",
    "        fit = RNN.fit(x_train_norm, y_train_norm, epochs = num_epochs, batch_size = batch_size, validation_split=0.2, callbacks=[early_stop, tensorboard], shuffle=True, use_multiprocessing=True)\n",
    "        loss_df = pd.DataFrame(fit.history)\n",
    "        print(RNN.summary())\n",
    "    elapsed = time.perf_counter() - start\n",
    "    print('Elapsed %.3f seconds.' % elapsed)\n",
    "    # Evaluate mode\n",
    "    # history = RNN.evaluate(x_test_norm, y_test_norm, batch_size = 1)\n",
    "\n",
    "    # Predicted Grade\n",
    "    predicted_Grade = RNN.predict(x_test_norm, verbose=1)\n",
    "    # predicted_Grade_Dataset = np.zeros(shape=(len(predicted_Grade), 10))\n",
    "    # predicted_Grade_Dataset[:,0] = predicted_Grade[:,0]\n",
    "    predicted_Grade = sc.inverse_transform(predicted_Grade)[:,0]\n",
    "    predicted_Grade = np.vstack((x_test_student, predicted_Grade))\n",
    "    predicted_Grade = predicted_Grade.T\n",
    "    predicted_Grade = pd.DataFrame(predicted_Grade)\n",
    "    \n",
    "    \n",
    "#     return predicted_Grade, loss_df\n",
    "\n",
    "    merged_Grades = student_cross_ref(student_list_unique, final_Grade, predicted_Grade)\n",
    "    #compute MAE & MSE\n",
    "    mae = tf.keras.losses.MeanAbsoluteError()\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    #write MAE from scratch, compare to tf???\n",
    "    mae = mae(merged_Grades.iloc[:,3],merged_Grades.iloc[:,1]).numpy()\n",
    "    mse = mse(merged_Grades.iloc[:,3],merged_Grades.iloc[:,1]).numpy()\n",
    "    #catch params\n",
    "    timestamp = time.strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "\n",
    "    path = r'C:\\Users\\Derrick\\Box\\Anon Data\\Undergrad Students\\Derrick Bailey\\NN Repository\\Graphs\\GRU\\Week 4\\\\'\n",
    "    path = path + timestamp + '.csv'\n",
    "\n",
    "    hype_list = pd.DataFrame(hype_list)\n",
    "    hype_list.to_csv(path, index=False)\n",
    "    visualize_chaos(merged_Grades, loss_df, mae, mse, fit, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Use demographics? Y/N:  N\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'x_train'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(232, 20)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'x_test'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(115, 38)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'y_train'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(232, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'y_test'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(115, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Optimize Hyperparameters? Y/N:  N\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_norm\n",
      "(232, 19, 1)\n",
      "[[[0.37433155]\n",
      "  [0.48663102]\n",
      "  [0.26724138]\n",
      "  ...\n",
      "  [0.18921708]\n",
      "  [1.        ]\n",
      "  [0.26331056]]\n",
      "\n",
      " [[0.26737968]\n",
      "  [0.38502674]\n",
      "  [0.21982759]\n",
      "  ...\n",
      "  [0.0963498 ]\n",
      "  [0.        ]\n",
      "  [0.40895175]]\n",
      "\n",
      " [[0.20855615]\n",
      "  [0.20855615]\n",
      "  [0.90086207]\n",
      "  ...\n",
      "  [0.01122146]\n",
      "  [0.        ]\n",
      "  [0.60955088]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.52941176]\n",
      "  [0.43315508]\n",
      "  [0.01293103]\n",
      "  ...\n",
      "  [0.00490133]\n",
      "  [0.        ]\n",
      "  [0.33817525]]\n",
      "\n",
      " [[0.77005348]\n",
      "  [0.73262032]\n",
      "  [0.00431034]\n",
      "  ...\n",
      "  [0.05855798]\n",
      "  [0.        ]\n",
      "  [0.40656291]]\n",
      "\n",
      " [[0.67379679]\n",
      "  [0.50802139]\n",
      "  [0.        ]\n",
      "  ...\n",
      "  [0.1883142 ]\n",
      "  [0.        ]\n",
      "  [0.56485097]]]\n",
      "x_test_norm\n",
      "(115, 37, 1)\n",
      "[[[0.11764706]\n",
      "  [1.        ]\n",
      "  [0.        ]\n",
      "  ...\n",
      "  [0.        ]\n",
      "  [1.        ]\n",
      "  [0.91299262]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.7037037 ]\n",
      "  [0.        ]\n",
      "  ...\n",
      "  [0.13333333]\n",
      "  [0.47368421]\n",
      "  [0.66243692]]\n",
      "\n",
      " [[0.23529412]\n",
      "  [1.        ]\n",
      "  [0.25      ]\n",
      "  ...\n",
      "  [0.4       ]\n",
      "  [1.        ]\n",
      "  [0.0863243 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.11764706]\n",
      "  [1.        ]\n",
      "  [0.08333333]\n",
      "  ...\n",
      "  [0.        ]\n",
      "  [1.        ]\n",
      "  [0.88234765]]\n",
      "\n",
      " [[0.05882353]\n",
      "  [0.92592593]\n",
      "  [0.        ]\n",
      "  ...\n",
      "  [0.        ]\n",
      "  [0.94736842]\n",
      "  [0.64017684]]\n",
      "\n",
      " [[0.17647059]\n",
      "  [1.        ]\n",
      "  [0.41666667]\n",
      "  ...\n",
      "  [0.46666667]\n",
      "  [1.        ]\n",
      "  [0.43686412]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[500, 1, 0.0001, 0.15, 0.15, 0.15, 0.2, 0.2, 0.2, 625, 125, 5]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 185 samples, validate on 47 samples\n",
      "Epoch 1/500\n",
      "185/185 [==============================] - 13s 73ms/sample - loss: 0.0824 - val_loss: 0.0365\n",
      "Epoch 2/500\n",
      "185/185 [==============================] - 9s 50ms/sample - loss: 0.0533 - val_loss: 0.0320\n",
      "Epoch 3/500\n",
      "185/185 [==============================] - 9s 51ms/sample - loss: 0.0513 - val_loss: 0.0274\n",
      "Epoch 4/500\n",
      "185/185 [==============================] - 9s 51ms/sample - loss: 0.0475 - val_loss: 0.0249\n",
      "Epoch 5/500\n",
      "174/185 [===========================>..] - ETA: 0s - loss: 0.0379WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "174/185 [===========================>..] - ETA: 0s - loss: 0.0379"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-c49a8983958b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#     predicted_Grade, loss_df = NeuralNetwork(x_train_norm, y_train_norm, x_test_norm, x_test_student, sc)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-c49a8983958b>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mx_train_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train_student\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test_student\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_Grade\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mhype_var_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimize_gate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhyperparameter_optimization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhyp_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Optimize Hyperparameters? Y/N: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mEvaluateModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhype_var_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimize_gate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test_student\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudent_list_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_Grade\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m#     predicted_Grade, loss_df = NeuralNetwork(x_train_norm, y_train_norm, x_test_norm, x_test_student, sc)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-dcf32a3907cb>\u001b[0m in \u001b[0;36mEvaluateModel\u001b[1;34m(hype_var_list, optimize_gate, x_train_norm, y_train_norm, x_test_norm, x_test_student, sc, student_list_unique, final_Grade)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;31m#         print(num_epochs, batch_size, lr, recurrent_dropout1, recurrent_dropout2, recurrent_dropout3, dropout1, dropout2, dropout3, gru_units1, gru_units2, gru_units3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mhype_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurrent_dropout1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurrent_dropout2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurrent_dropout3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgru_units1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgru_units2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgru_units3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mBuildModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test_student\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhype_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudent_list_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_Grade\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;31m#         pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-fa095b4cfafc>\u001b[0m in \u001b[0;36mBuildModel\u001b[1;34m(x_train_norm, y_train_norm, x_test_norm, x_test_student, sc, hype_list, student_list_unique, final_Grade)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;31m# Fitting RNN to training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mfit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[0mloss_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project_boxsand_neural_network\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project_boxsand_neural_network\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project_boxsand_neural_network\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project_boxsand_neural_network\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project_boxsand_neural_network\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project_boxsand_neural_network\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project_boxsand_neural_network\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project_boxsand_neural_network\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project_boxsand_neural_network\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project_boxsand_neural_network\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project_boxsand_neural_network\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    train_data, test_data, final_Grade, studentdf, student_list_unique = import_data()\n",
    "    x_train_norm, x_test_norm, y_train_norm, y_test_norm, x_train_student, x_test_student, sc = process_data(train_data, test_data, final_Grade)\n",
    "    hype_var_list, optimize_gate = hyperparameter_optimization(hyp_opt = input(\"Optimize Hyperparameters? Y/N: \"))\n",
    "    EvaluateModel(hype_var_list, optimize_gate, x_train_norm, y_train_norm, x_test_norm, x_test_student, sc, student_list_unique, final_Grade)\n",
    "#     predicted_Grade, loss_df = NeuralNetwork(x_train_norm, y_train_norm, x_test_norm, x_test_student, sc)\n",
    "    \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
